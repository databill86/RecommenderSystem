{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the import\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from caserec.utils.split_database import SplitDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import clear_output\n",
    "\n",
    "lib_path = './../Sources/Utilities'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "\n",
    "from messaging.print_functions import ProgressBar\n",
    "from messaging.telegrambot import Bot\n",
    "bot = Bot(user_credentials='./JFGS.json')\n",
    "\n",
    "# Checking if bot is ok\n",
    "bot.send_message(text=\"Hello, John\")\n",
    "progbar = ProgressBar(bar_length=20, bar_fill='#', elapsed_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, dataset_type = 'MovieLens', '100k'\n",
    "# dataset, dataset_type = 'BookCrossing', 'Standard'\n",
    "# dataset, dataset_type = 'Amazon', 'MoviesTV'\n",
    "dataset, dataset_type = 'Amazon', 'InstantVideo'\n",
    "# dataset, dataset_type = 'Jester', 'jester'\n",
    "\n",
    "dataset_folder = \"../Datasets/\" + dataset + \"/\" + dataset_type + \"/\"\n",
    "dataset_output_folder = dataset_folder + 'outputs/'\n",
    "\n",
    "df_overall_sparsity = pd.read_csv(dataset_output_folder + 'df_overall_sparsity.tsv', sep='\\t', header=0)\n",
    "df_overall_sparsity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_datasets_folder = dataset_output_folder + 'sparsity_datasets/'\n",
    "sparsity_folders = os.listdir(sparsity_datasets_folder)\n",
    "if 'desktop.ini' in sparsity_folders:    \n",
    "    sparsity_folders.remove('desktop.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(**kwargs):\n",
    "    \n",
    "    if (kwargs['model_name'].lower() == 'item-knn'):\n",
    "        from caserec.recommenders.item_recommendation.itemknn import ItemKNN\n",
    "\n",
    "        model = ItemKNN(\n",
    "                        train_file=kwargs['train_filepath'], \n",
    "                        test_file=kwargs['test_filepath'], \n",
    "        #                 as_binary=True, # If True, the explicit feedback will be transform to binary\n",
    "                        k_neighbors=kwargs['k_neighbors'],\n",
    "                        similarity_metric=kwargs['similarity_metric'],\n",
    "                        rank_length=kwargs['top_n'])\n",
    "\n",
    "        model.compute(\n",
    "            metrics=None, \n",
    "            as_table=True,\n",
    "            n_ranks=[kwargs['top_n']],\n",
    "            verbose=False)\n",
    "        \n",
    "        eval_results = model.evaluation_results\n",
    "\n",
    "    elif (kwargs['model_name'].lower() == 'nnmf'):  \n",
    "        from caserec.recommenders.rating_prediction.nnmf import NNMF\n",
    "        from caserec.utils.process_data import ReadFile\n",
    "        from caserec.evaluation.rating_prediction import RatingPredictionEvaluation\n",
    "        \n",
    "        predictions_output_filepath = './predictions_output.dat'\n",
    "        \n",
    "        model = NNMF(kwargs['train_filepath'], \n",
    "                      kwargs['test_filepath'], \n",
    "                      factors=kwargs['n_factors'],\n",
    "                     output_file = predictions_output_filepath)\n",
    "        \n",
    "        model.compute(verbose=False)\n",
    "        \n",
    "        # Using ReadFile class to read predictions from file\n",
    "        reader = ReadFile(input_file=predictions_output_filepath)\n",
    "        predictions = reader.read()\n",
    "        \n",
    "        # Creating evaluator with item-recommendation parameters\n",
    "        evaluator = RatingPredictionEvaluation(sep = '\\t', \n",
    "                                               n_rank = [kwargs['top_n']], \n",
    "                                               as_rank = True,\n",
    "                                               metrics = list(kwargs['metrics']))\n",
    "\n",
    "        # Getting evaluation\n",
    "        eval_results = evaluator.evaluate(predictions['feedback'], model.test_set)\n",
    "        \n",
    "        \n",
    "    elif (kwargs['model_name'].lower() == 'bprmf'):  \n",
    "        from caserec.recommenders.item_recommendation.bprmf import BprMF\n",
    "        \n",
    "        model = BprMF(kwargs['train_filepath'], \n",
    "                      kwargs['test_filepath'], \n",
    "                      batch_size=30, \n",
    "                      rank_length = kwargs['top_n'])\n",
    "        \n",
    "        model.compute(\n",
    "            metrics=None, \n",
    "            as_table=True)\n",
    "        \n",
    "        eval_results = model.evaluation_results\n",
    "\n",
    "    eval_results['uss_limit'] = kwargs['uss_limit']\n",
    "    eval_results['iss_limit'] = kwargs['iss_limit']\n",
    "\n",
    "    return eval_results    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of argument to evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "k_neighbors = 30\n",
    "n_factors = 30\n",
    "similarity_metric= 'cosine'\n",
    "metrics = ('PREC', 'RECALL', 'NDCG', 'MAP')\n",
    "model_name = 'nnmf'\n",
    "\n",
    "arr_eval_args = []\n",
    "\n",
    "for index, row in df_overall_sparsity.iterrows():    \n",
    "    progbar.update_progress(index/float(df_overall_sparsity.shape[0]))\n",
    "        \n",
    "#     if (index > 15):\n",
    "#         break\n",
    "        \n",
    "    fold_num = 0\n",
    "    n_folds = 2\n",
    "    \n",
    "    uss_limit = row['uss_limit']\n",
    "    iss_limit = row['iss_limit']\n",
    "    \n",
    "    target_folder = 'usslimit_{}_isslimit_{}'.format(uss_limit, iss_limit)\n",
    "\n",
    "    if target_folder not in sparsity_folders:\n",
    "        print (\"Error findind \" + target_folder + \" folder\")    \n",
    "        break\n",
    "    else:        \n",
    "        # Visualize file content\n",
    "        ratings_filepath = sparsity_datasets_folder + target_folder + '/u.data'\n",
    "        cross_validation_folder = sparsity_datasets_folder + target_folder + '/'\n",
    "\n",
    "        try:\n",
    "        \n",
    "            SplitDatabase(input_file=ratings_filepath, dir_folds = cross_validation_folder, n_splits=n_folds).k_fold_cross_validation()\n",
    "\n",
    "        except : \n",
    "            print (\"Erro em \", ratings_filepath)\n",
    "            break\n",
    "            \n",
    "        fold_path = cross_validation_folder + 'folds/' + str(fold_num)\n",
    "\n",
    "        train_filepath = fold_path + '/train.dat'\n",
    "        test_filepath = fold_path + '/test.dat'\n",
    "\n",
    "        temp_eval_args = {'uss_limit': uss_limit,\n",
    "                          'iss_limit': iss_limit,\n",
    "                          'model_name': model_name, \n",
    "                          'train_filepath': train_filepath, \n",
    "                          'test_filepath': test_filepath, \n",
    "                          'top_n': top_n, \n",
    "#                           'k_neighbors': k_neighbors, \n",
    "                          'n_factors': n_factors,\n",
    "                          'similarity_metric': similarity_metric,\n",
    "                          'metrics': metrics}\n",
    "\n",
    "        arr_eval_args.append(temp_eval_args)\n",
    "\n",
    "# del sparsity_folders   \n",
    "del df_overall_sparsity\n",
    "del sparsity_folders        \n",
    "text = \"Finished creating arr_eval_args for {} on {}/{} dataset in {}\".format(model_name, dataset, dataset_type, progbar.get_elapsed_time())\n",
    "bot.send_message(text=text)\n",
    "joblib.dump(arr_eval_args, dataset_output_folder + 'arr_eval_args_' + model_name + '.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../Datasets/Amazon/InstantVideo/outputs/sparsity_datasets/usslimit_1.0_isslimit_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_eval_args[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(**arr_eval_args[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(arr_eval_args, dataset_output_folder + 'arr_eval_args_' + model_name + '.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_eval_args = joblib.load(dataset_output_folder + 'arr_eval_args_' + model_name + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_evals = len(arr_eval_args)\n",
    "for index, row in enumerate(arr_eval_args):\n",
    "    current_save = int(10*index/float(max_evals)) \n",
    "    \n",
    "    clear_output()\n",
    "    update_progress(index/float(max_evals))    \n",
    "\n",
    "    print (row)\n",
    "    \n",
    "#     if (index >= 15):\n",
    "#         break\n",
    "    \n",
    "    eval_result = eval_model(**arr_eval_args[index])\n",
    "\n",
    "    if (index == 0):\n",
    "        colnames = list(eval_result.keys())\n",
    "        df_eval_metadata = pd.DataFrame(columns = colnames)\n",
    "        \n",
    "    df_eval_metadata.loc[index] = [eval_result[x] for x in colnames]\n",
    "\n",
    "text = \"Finished creating df_eval_metadata for {} on {}/{} dataset\".format(model_name, dataset, dataset_type)\n",
    "bot.sendMessage(bot_credentials['chat_id'], text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_metadata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits = df_eval_metadata['uss_limit'].unique()\n",
    "iss_limits = df_eval_metadata['iss_limit'].unique()\n",
    "\n",
    "# column = 'MAP@10'\n",
    "\n",
    "for column in ['PREC@' + str(top_n), 'RECALL@' + str(top_n), 'NDCG@' + str(top_n), 'MAP@' + str(top_n)]:\n",
    "\n",
    "    arr_prec = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "    for uss_index, uss_limit in enumerate(uss_limits):\n",
    "        for iss_index, iss_limit in enumerate(iss_limits):\n",
    "            arr_prec[uss_index, iss_index] = df_eval_metadata[(df_overall_sparsity['uss_limit'] == uss_limit) & (df_overall_sparsity['iss_limit'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "\n",
    "    joblib.dump(arr_prec, dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['PREC@' + str(top_n), 'RECALL@' + str(top_n), 'NDCG@' + str(top_n), 'MAP@' + str(top_n)]:\n",
    "\n",
    "    arr_prec = joblib.load(dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        \n",
    "\n",
    "    cmapping = \"jet\"\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # plt.subplot(1, 1, 1)\n",
    "    cax = plt.imshow(arr_prec, cmap=cmapping)\n",
    "    plt.gca().invert_yaxis()\n",
    "    cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "    # plt.colorbar.set_label('OS', labelpad=-50,  y=1.05, rotation=0, fontsize = label_fontsize)\n",
    "\n",
    "    tick_step = int(10)\n",
    "\n",
    "    ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "    ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "\n",
    "    plt.xticks(np.arange(0, len(uss_limits), tick_step))\n",
    "    plt.yticks(np.arange(0, len(iss_limits), tick_step))\n",
    "\n",
    "    plt.clim(0, 1)\n",
    "\n",
    "\n",
    "    tick_fontsize = 20\n",
    "    label_fontsize = 25\n",
    "\n",
    "    # # Setting Labels\n",
    "    ax.set_xlabel('Last User Specific Sparsity', fontsize = label_fontsize)\n",
    "    ax.set_ylabel('Last Item Specific Sparsity', fontsize = label_fontsize)\n",
    "    # cbar = fig.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,5)])\n",
    "\n",
    "    plt.xticks(rotation = 'vertical')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "    cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = label_fontsize)\n",
    "    cbar.ax.tick_params(labelsize = tick_fontsize)\n",
    "\n",
    "\n",
    "    # Saving figure\n",
    "    filename = '2d-' + column + '-' + model_name + '.png';\n",
    "    fullpath = dataset_output_folder+'Figures/';\n",
    "    print (\"[*] Saving \" + filename + \" figure to \" + fullpath + \" folder...\")\n",
    "    fig.savefig(fullpath + filename, bbox_inches = 'tight')\n",
    "    print (\"[+] Results saved.\")\n",
    "    \n",
    "    bot.send_photo(bot_credentials['chat_id'], photo=open(fullpath + filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "pool = Pool(processes=4)              # start 4 worker processes\n",
    "\n",
    "# # print \"[0, 1, 4,..., 81]\"\n",
    "print (pool.map(f, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(**temp_eval_args)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"{}ratings.csv\".format(folder)\n",
    "# output_folder = '../Datasets/' + dataset + '/' + dataset_type + '/outputs/'\n",
    "# dataset_output_folder = output_folder + 'sparsity_dataset/'\n",
    "\n",
    "# # Visualize file content\n",
    "# # df_whole = pd.read_csv(filepath, sep='\\t', header=0, names=['user_id', 'item_id', 'rating', 'timestamp']) \n",
    "# df_whole = pd.read_csv(filepath, sep='\\t', header=0) \n",
    "# df_whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(dataset_output_folder):\n",
    "    os.makedirs(dataset_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "pool = Pool(processes=4)              # start 4 worker processes\n",
    "\n",
    "# print \"[0, 1, 4,..., 81]\"\n",
    "print (pool.map(f, range(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
